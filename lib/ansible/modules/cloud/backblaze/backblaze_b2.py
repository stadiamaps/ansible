#!/usr/bin/python3

# Copyright: (c) 2019, Ian Wagner <ian.wagner@stadiamaps.com>
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)

ANSIBLE_METADATA = {
    'metadata_version': '1.1',
    'status': ['preview'],
    'supported_by': 'community'
}

DOCUMENTATION = '''
---
module: b2

short_description: Utilities for working with Backblaze B2 cloud storage

version_added: "2.8"

description:
    - "This module allows you to automate basic actions such as uploading and downloading files with Backblaze B2."

options:
    key_id:
        description:
            - Your application key ID (please don't use a master key). This key should have permission to list files
            - in the chosen bucket, for the purpose of purging incomplete uploads for files of the same name on upload.
        required: true
    application_key:
        description:
            - Your application key (please don't use a master key).
        required: true
    bucket_id:
        description:
            - The ID of the bucket to operate on.
        required: true
    action:
        description:
            - The action to perform.
        choices:
            - upload
        required: true
    src:
        description:
            - The local source file (when uploading).
        required: false

requirements:
    - requests
    - "python >= 3.4"

extends_documentation_fragment:
    - backblaze

author:
    - Ian Wagner (@ianthetechie)
'''

EXAMPLES = '''
- name: Upload a file to B2
  backblaze_b2:
    bucket_id: '{{ bucket_id }}'
    src: '{{ file_path }}'
    action: upload
    key_id: '{{ b2_key_id }}'
    application_key: '{{ b2_application_key }}'
'''

RETURN = '''
msg:
    description: Informational output about the result of the operation
    type: str
details:
    description: Data about what went wrong, in the case of an error
    type: dict
'''

# TODO: See if we can support Python 2.7 as well

import collections
import hashlib
import io
import os
import os.path
import queue
import threading
import time
import urllib.parse

import requests
import requests.adapters

from typing import Dict, Optional
from urllib.parse import urljoin

from ansible.module_utils.basic import AnsibleModule
from urllib3.util.retry import Retry

B2_API_HOST = 'api.backblazeb2.com'

# TODO: Make these params?
SHA_BUFFER_SIZE = io.DEFAULT_BUFFER_SIZE
LARGE_FILE_UPLOAD_THREADS = 4  # The number of parallel upload threads to use per file.


def compute_file_sha1(path: str, chunk_size: Optional[int] = None) -> (str, [str]):
    """Returns a tuple containing the full file sha1, and a list of chunk sha1s.

    The chunk sha1 list will be empty unless you pass in a chunk_size. This is useful
    for backblaze large file uploads.
    """
    full_sha1 = hashlib.sha1()
    chunk_sha1 = hashlib.sha1()
    chunk_sha1s = []

    with open(path, 'rb') as fp:
        read_buffer_size = SHA_BUFFER_SIZE if chunk_size is None or SHA_BUFFER_SIZE % chunk_size == 0 else chunk_size
        ptr = 0

        while True:
            data = fp.read(read_buffer_size)
            ptr += read_buffer_size

            if data:
                full_sha1.update(data)

                if chunk_size:
                    chunk_sha1.update(data)

                    if ptr % chunk_size == 0:
                        chunk_sha1s.append(chunk_sha1.hexdigest())
                        chunk_sha1 = hashlib.sha1()
            else:
                return full_sha1.hexdigest(), chunk_sha1s


class B2Error(Exception):
    """An exception thrown by this module.

    The message is a pretty string generated by this module. The details
    are the raw JSON info returned from the B2 API (if relevant).
    """

    def __init__(self, message: str, details: Optional[dict]):
        self.message = message
        self.details = details


class B2Token:
    def __init__(self, authorization_token: str, api_url: str, download_url: str, recommended_part_size: str):
        self.authorization_token = authorization_token
        self.api_url = api_url
        self.download_url = download_url
        self.recommended_part_size = int(recommended_part_size)


class B2Session(requests.Session):
    """A session that makes requests to Backblaze B2 and handles token creation and expiry events."""

    def __init__(self, key_id: str, app_key: str, **kwargs):
        super().__init__(**kwargs)

        self.key_id = key_id
        self.app_key = app_key

        # Dequeue chosen for it's combination of speed and thread safety.
        # This collection does not need the concurrency operations that the task queue does.
        self.__authorization_tokens = collections.deque()

        b2_retries = Retry(total=3, backoff_factor=1, method_whitelist=False)
        self.mount('https://', requests.adapters.HTTPAdapter(max_retries=b2_retries))

    def __dequeue_token(self) -> B2Token:
        try:
            return self.__authorization_tokens.popleft()
        except IndexError:
            return self.__generate_token()

    def __generate_token(self):
        auth_info = self.key_id, self.app_key
        r = requests.get('https://{0:s}/b2api/v1/b2_authorize_account'.format(B2_API_HOST), auth=auth_info)

        if r.status_code == 200:
            response_data = r.json()

            return B2Token(response_data['authorizationToken'],
                           response_data['apiUrl'],
                           response_data['downloadUrl'],
                           response_data['recommendedPartSize'])
        else:
            raise requests.RequestException('Error in authorize_account. Unable to authorize account: %s', r.json())

    @property
    def recommended_part_size(self):
        token = self.__dequeue_token()
        result = token.recommended_part_size
        self.__authorization_tokens.append(token)

        return result

    def request(self, method, path, headers=None, **kwargs):
        """This overloaded session method operates on a path, not a URL argument.

        Do NOT pass a full URL except when uploading. The correct URL will be constructed automatically in most cases.
        Authorization headers will be set if necessary. In the case of uploads, you will need to set a custom
        Authorization header due to the peculiarities of how b2_upload_file works.

        The final URL is constructed based on the value of some optional keyword arguments. For a file download
        request, pass download=True to the session get method. To make a call to the absolute URL passed in rather
        than treating it as a fragment (this is necessary for uploads), pass absolute=True.
        For all other requests, no special arguments are needed.
        """
        download = kwargs.pop('download', False)
        absolute = kwargs.pop('absolute', False)
        retried = kwargs.pop('retried', False)

        token = self.__dequeue_token()

        headers = headers or {}

        if 'Authorization' not in headers:
            headers['Authorization'] = token.authorization_token

        if absolute:
            url = path
        elif download:
            url = urljoin(token.download_url, path)
        else:
            url = urljoin(token.api_url, path)

        response = super().request(method, url, headers=headers, **kwargs)

        if response.status_code != 401:
            # Success! Return the token to the pool if necessary
            if token:
                self.__authorization_tokens.append(token)
        elif response.status_code == 401 and not retried and token is not None:
            # Retry the request (burning the old token)
            return self.request(method, path, headers, retried=True, absolute=absolute, download=download, **kwargs)

        return response


class B2File:
    def __init__(self,
                 file_id: str,
                 file_name: str,
                 content_length: int,
                 content_type: str,
                 content_sha1: str,
                 file_info: dict,
                 upload_timestamp: int):
        self.file_id = file_id
        self.file_name = file_name
        self.content_length = content_length
        self.content_type = content_type
        self.content_sha1 = content_sha1
        self.file_info = file_info
        self.upload_timestamp = upload_timestamp


class B2Bucket:
    def __init__(self, bucket_id: str, bucket_name: str):
        self.bucket_id = bucket_id
        self.bucket_name = bucket_name


class B2UploadAuthorization:
    def __init__(self, bucket_id: str, upload_url: str, authorization_token: str):
        self.bucket_id = bucket_id
        self.upload_url = upload_url
        self.authorization_token = authorization_token


class B2Client:
    def __init__(self, key_id: str, app_key: str, bucket_id: str):
        """Initializes a new client for interacting with the B2 API."""
        self.bucket_id = bucket_id
        self.__upload_auth = None

        self.__session = B2Session(key_id, app_key)
        self.__recommended_part_size = self.__session.recommended_part_size

        self.__known_unfinished_files = self.list_unfinished_large_files()

    @staticmethod
    def parse_file_record(rec: dict) -> B2File:
        return B2File(rec['fileId'],
                      rec['fileName'],
                      rec['contentLength'],
                      rec['contentType'],
                      rec.get('contentSha1'),  # May be missing for an incomplete file
                      rec.get('fileInfo'),
                      rec['uploadTimestamp'] // 1000)  # Note that B2 returns integer milliseconds

    def list_unfinished_large_files(self) -> Dict[str, str]:
        """Returns a dictionary of unfinished large files.

        The key is the file name, and the value is the file ID. This ensures easy look-ups.
        """
        params = {
            'bucketId': self.bucket_id,
        }

        r = self.__session.get('/b2api/v1/b2_list_unfinished_large_files', params=params)

        if r.status_code == 200:
            return {rec['fileName']: rec['fileId'] for rec in r.json()['files']}
        else:
            print(r.json())
            raise B2Error('Error in b2_list_unfinished_large_files.', r.json())

    def cancel_large_file(self, file_id: str):
        params = {
            'fileId': file_id,
        }

        r = self.__session.get('/b2api/v1/b2_cancel_large_file', params=params)

        if r.status_code != 200:
            raise B2Error('Error in b2_cancel_large_file.', r.json())

    def hide_file(self, file_name: str):
        params = {
            'bucketId': self.bucket_id,
            'fileName': file_name,
        }

        r = self.__session.get('/b2api/v1/b2_hide_file', params=params)

        if r.status_code != 200:
            raise B2Error('Error in b2_hide_file.', r.json())

    def get_upload_auth(self, bucket_id: str) -> B2UploadAuthorization:
        params = {
            'bucketId': bucket_id
        }

        r = self.__session.get('/b2api/v1/b2_get_upload_url', params=params)

        if r.status_code == 200:
            response_data = r.json()
            return B2UploadAuthorization(bucket_id=response_data['bucketId'],
                                         upload_url=response_data['uploadUrl'],
                                         authorization_token=response_data['authorizationToken'])
        else:
            raise B2Error('Error in b2_get_upload_url.', r.json())

    def upload_file(self, local_path: str, upload_filename: str):
        # Try to get an upload URL
        num_failures = 0
        while self.__upload_auth is None:
            if num_failures > 5:
                raise B2Error('Failed to get an upload URL after 5 attempts. Giving up.', {})

            self.__upload_auth = self.get_upload_auth(self.bucket_id)

            if self.__upload_auth is None:
                num_failures += 1
                time.sleep(3)  # Back off for a sec

        try:
            file_info = os.stat(local_path)
        except FileNotFoundError:
            raise B2Error('File does not exist', {'path': local_path})

        file_size = file_info.st_size
        is_large_file = file_size > self.__recommended_part_size

        if is_large_file:  # Large files are handled differently
            # Compute the part size to use based on the file size
            part_size = self.__recommended_part_size if file_size > self.__recommended_part_size * 2 else self.__recommended_part_size // 2
            thread_count = min(LARGE_FILE_UPLOAD_THREADS, file_size // part_size)

            existing_file_id = self.__known_unfinished_files.get(upload_filename)
            if existing_file_id:
                # Nuke it and start afresh
                self.cancel_large_file(existing_file_id)

            # Get the process rolling with B2
            full_sha1, chunk_sha1s = compute_file_sha1(local_path, part_size)
            start_params = {
                'bucketId': self.bucket_id,
                'fileName': upload_filename,
                'contentType': 'b2/x-auto',
                'fileInfo': {
                    'large_file_sha1': full_sha1,
                    'src_last_modified_millis': str(int(file_info.st_mtime * 1000)),  # They want a stringified int.
                },
            }
            start_response = self.__session.post('/b2api/v1/b2_start_large_file', json=start_params)
            data = start_response.json()

            if start_response.status_code == 200:
                file_id = data['fileId']

                upload_part_url_params = {
                    'fileId': file_id
                }

                # Set up a pool of URLs and tokens to use when uploading the file in parallel
                url_token_pool = collections.deque()
                for _ in range(thread_count):
                    token_response = self.__session.post('/b2api/v1/b2_get_upload_part_url',
                                                         json=upload_part_url_params)

                    data = token_response.json()
                    if token_response.status_code == 200:
                        url_token_pool.append(data)
                    else:
                        raise B2Error('Error retrieving token in b2_get_upload_part_url.', data)

                task_queue = queue.Queue()
                failed_tasks = collections.deque()

                def worker():
                    while True:
                        task = task_queue.get()  # Get a job from the queue
                        url_token = None

                        try:
                            if task is None:  # None is used as a sentinel that we are done
                                task_queue.task_done()
                                break
                            else:
                                part_number, part_sha1 = task
                                url_token = url_token_pool.popleft()

                                # Finally, we can upload stuff!
                                upload_headers = {
                                    'Authorization': url_token['authorizationToken'],
                                    'X-Bz-Part-Number': str(part_number + 1),  # These must be one-indexed
                                    'X-Bz-Content-Sha1': part_sha1,
                                }

                                with open(local_path, 'rb') as fp:
                                    fp.seek(part_number * part_size)  # Read the correct section
                                    upload_response = self.__session.post(url_token['uploadUrl'],
                                                                          data=fp.read(part_size),
                                                                          headers=upload_headers)

                                    if upload_response.status_code != 200:
                                        raise B2Error('Error uploading file part', upload_response.json())
                        except:
                            failed_tasks.append(task)
                        finally:
                            if url_token is not None:
                                url_token_pool.append(url_token)

                            if task is not None:
                                task_queue.task_done()  # Mark the task as done

                # Set up a worker pool
                worker_threads = []
                for _ in range(thread_count):
                    t = threading.Thread(target=worker)
                    t.start()

                    worker_threads.append(t)

                for i, sha in enumerate(chunk_sha1s):
                    task_queue.put((i, sha))

                # Put in ending sentinel jobs
                for _ in worker_threads:
                    task_queue.put(None)

                task_queue.join()

                for thread in worker_threads:
                    thread.join()

                if len(failed_tasks) == 0:
                    # Finish file
                    finish_params = {
                        'fileId': file_id,
                        'partSha1Array': chunk_sha1s,
                    }
                    finish_response = self.__session.post('/b2api/v1/b2_finish_large_file',
                                                          json=finish_params)

                    if finish_response.status_code != 200:
                        B2Error('Error in b2_finish_large_file.', finish_response.json())
                else:
                    raise B2Error('Failed to upload file %s because one or more chunks failed.' % upload_filename, {})
            else:
                raise B2Error('Error in b2_start_large_file. Response: %s', data)
        else:
            headers = {
                'Authorization': self.__upload_auth.authorization_token,
                'X-Bz-File-Name': urllib.parse.quote(upload_filename.encode('utf-8')),
                'Content-Type': 'b2/x-auto',
                'X-Bz-Content-Sha1': compute_file_sha1(local_path)[0]
            }

            with open(local_path, 'rb') as fp:
                r = self.__session.post(self.__upload_auth.upload_url, data=fp.read(), headers=headers)

                if r.status_code != 200:
                    raise B2Error('Error in b2_upload_file. Response: %s', r.json())

    def delete_file(self, filename: str):
        # TODO: Change this hehe... this is not valid for all use cases
        self.hide_file(filename)


def run_module():
    # define available arguments/parameters a user can pass to the module
    module_args = {
        'key_id': {'type': 'str', 'required': True},
        'application_key': {'type': 'str', 'required': True},
        'bucket_id': {'type': 'str', 'required': True},
        'action': {'choices': ['upload'], 'required': True},
        'src': {'type': 'str', 'required': False}
    }

    # seed the result dict in the object
    # we primarily care about changed and state
    # change is if this module effectively modified the target
    # state will include any data that you want your module to pass back
    # for consumption, for example, in a subsequent task
    result = {'changed': False, 'msg': '', 'details': {}}

    # the AnsibleModule object will be our abstraction working with Ansible
    # this includes instantiation, a couple of common attr would be the
    # args/params passed to the execution, as well as if the module
    # supports check mode
    module = AnsibleModule(
        argument_spec=module_args,
        supports_check_mode=True,
        required_if=[
            ['action', 'upload', ['src']],
        ]
    )

    try:
        client = B2Client(module.params['key_id'], module.params['application_key'], module.params['bucket_id'])

        # In check mode, just make sure we could create a B2Client, which does some basic authentication
        if module.check_mode:
            result['msg'] = 'B2 authentication succeeded; no operation will be performed in check mode.'
            result['changed'] = True
            return result

        # Perform operations
        action = module.params['action']
        if action == 'upload':
            src = module.params['src']
            upload_filename = os.path.basename(src)  # TODO: Support this via an option as well
            client.upload_file(src, upload_filename)

            result['msg'] = 'File uploaded successfully.'
            result['changed'] = True

        module.exit_json(**result)
    except B2Error as e:
        result['msg'] = e.message
        result['details'] = e.details

        module.fail_json(**result)


def main():
    run_module()


if __name__ == '__main__':
    main()
